{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../.env\")\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI-API-KEY')\n",
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "import time\n",
    "\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prevents the metadata from being None which causes errors with the vectorstore\n",
    "def sanitize_metadata(data):\n",
    "    for item in data:\n",
    "        meta = item.metadata\n",
    "        for key, value in meta.items():\n",
    "            if value is None:\n",
    "                meta[key] = \"\"\n",
    "    return data\n",
    "\n",
    "def unpack (data):\n",
    "    return [{'page' : idx + 1, 'content' : page.page_content, 'metadata' : page.metadata} for idx, page in enumerate(data)]\n",
    "\n",
    "def dot_product_similarity(doc_data: List[Dict], query_data: Dict) -> List[Tuple[int, float]]:\n",
    "    query_embedding = query_data['embedding']\n",
    "    doc_embeddings = [page['embedding'] for page in doc_data]\n",
    "    tuples_list = [(page['page'], np.dot(query_embedding, embedding)) for page, embedding in zip(doc_data, doc_embeddings)]\n",
    "    ordered_tuples = sorted(tuples_list, key=itemgetter(1), reverse=True)\n",
    "    top_five_tuples = ordered_tuples[:5]\n",
    "    return top_five_tuples\n",
    "\n",
    "def embed_query(query: str):\n",
    "    base_embeddings = OpenAIEmbeddings()\n",
    "    embedding = base_embeddings.embed_query(query)\n",
    "    return {\"query\" : query, \"embedding\" : embedding}\n",
    "\n",
    "def embed_doc(text_list : List[str]):\n",
    "    base_embeddings = OpenAIEmbeddings()\n",
    "    doc_embeddings = base_embeddings.embed_documents(text_list)\n",
    "    return doc_embeddings\n",
    "\n",
    "def get_context(query_data, doc_data: List[Dict]) -> List[Dict]:\n",
    "    top_five_tuples = dot_product_similarity(doc_data, query_data)\n",
    "    context = []\n",
    "    for item in top_five_tuples:\n",
    "        page = item[0]\n",
    "        data = {'page': page, 'similarity' : item[1], 'text': doc_data[page - 1]['content'], 'metadata' : doc_data[page - 1]['metadata']}\n",
    "        context.append(data)\n",
    "    return context\n",
    "\n",
    "def get_tokens(string: str, model: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def format_context(context: List[Dict], model: str, token_limit : int) -> str:\n",
    "    \"\"\"Returns a string of the first 1024 tokens of the context.\"\"\"\n",
    "    context_string = \"\"\n",
    "    meta_list = []\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    for idx, item in enumerate(context):\n",
    "        sanitized_text = item['text'].replace(\"\\n\", \" \")\n",
    "        context_string += f\"Page: {item['page']}\\n\\nText: {sanitized_text}\\n\\n\"\n",
    "        meta_list.append(item['metadata'])\n",
    "        tokens = get_tokens(context_string, model)\n",
    "        if tokens > token_limit:\n",
    "            encoded_text = encoding.encode(context_string)\n",
    "            # cut it down to the token limit\n",
    "            encoded_text = encoded_text[:token_limit]\n",
    "            # decode it back to a string\n",
    "            context_string = encoding.decode(encoded_text)\n",
    "            # some testing to make sure it worked\n",
    "            tokens = get_tokens(context_string, model)\n",
    "            assert tokens <= token_limit, f\"format context function failed to cut context down far enough. tokens: {tokens}\"\n",
    "            break\n",
    "    return context_string, meta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_text(text):\n",
    "    # Replace any non-alphanumeric character with a space\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Replace any multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Completion:\n",
    "    def __init__(self, temperature, max_tokens, stream=False, model=\"text-davinci-003\", **kwargs):\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.stream = stream\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        raw_response = openai.Completion.create(\n",
    "            model=self.model,\n",
    "            prompt=text,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        if self.stream:\n",
    "            return raw_response\n",
    "        else:\n",
    "            return raw_response['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self, temperature, system_message=\"You are a helpful assistant.\", messages=None, model='gpt-3.5-turbo'):\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        if messages is not None:\n",
    "            self.messages += [{\"role\": \"user\", \"content\": message} for message in messages]\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    def __call__(self, user_message: str):\n",
    "        user_message = {\"role\": \"user\", \"content\": user_message}\n",
    "        self.messages.append(user_message)\n",
    "        raw_response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        response_message = raw_response['choices'][0]['message']['content'].strip()\n",
    "        self.messages.append(response_message)\n",
    "        return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mongodoc:\n",
    "    def __init__(self, fpath : str):\n",
    "        self.fpath = fpath\n",
    "\n",
    "    def process_doc(self):\n",
    "        loader = PyMuPDFLoader(self.fpath)\n",
    "        # load the data\n",
    "        unsanitized = loader.load()\n",
    "        # make sure the metadata is not None\n",
    "        data = sanitize_metadata(unsanitized)\n",
    "        # get the doc embeddings\n",
    "        doc_embeddings = embed_doc([page.page_content for page in data])\n",
    "        # unpack the data and add the embeddings\n",
    "        mongodoc = unpack(data)\n",
    "        mongodoc = [{**page, \"embedding\": embedding} for page, embedding in zip(mongodoc, doc_embeddings)]\n",
    "\n",
    "        self.data = mongodoc\n",
    "        self.page_text = ' '.join([sanitize_text(page['content']) for page in mongodoc])\n",
    "        self.metadata = [page['metadata'] for page in mongodoc]\n",
    "        return self\n",
    "    \n",
    "    def get_chunks(self, chunk_size : int):\n",
    "        enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "        tokens = enc.encode(self.page_text)\n",
    "        # split into chunks of 2800 tokens\n",
    "        chunks = [tokens[i:i+2800] for i in range(0, len(tokens), 2800)]\n",
    "        # decode chunks\n",
    "        decoded = [enc.decode(chunk) for chunk in chunks]\n",
    "        self.chunks = decoded\n",
    "        return self\n",
    "    \n",
    "    def get_citation(self, format: str):\n",
    "        citation_chat = Chat(temperature=0.9)\n",
    "        get_citation_prompt = f\"Use this metadata to generate a ciation in {format} format: \\n\\n{self.metadata}\"\n",
    "        final_citation = citation_chat(get_citation_prompt)\n",
    "        self.citation = final_citation\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chain:\n",
    "    def __init__(self, mongodoc: Mongodoc):\n",
    "        self.mongodoc = mongodoc\n",
    "        # pass all the attributes from the mongodoc to the chain\n",
    "        for attr in dir(mongodoc):\n",
    "            if not callable(getattr(mongodoc, attr)) and not attr.startswith(\"__\"):\n",
    "                setattr(self, attr, getattr(mongodoc, attr))\n",
    "\n",
    "    def link_1(self):\n",
    "        print(\"Starting Link 1:\")\n",
    "        citation_qualifier = f\"Use this citation: {self.citation} to cite your work.\"\n",
    "        main_ideas_prompt = f\"Identify and list 2-3 main ideas from the context. {citation_qualifier}\"\n",
    "        quotes_prompt = f\"Identify and list 2-3 relevant quotes from the context. {citation_qualifier}\"\n",
    "        passages_prompt = f\"Identify and list 2-3 relevant passages from the context. {citation_qualifier}\"\n",
    "\n",
    "        prompts = [\n",
    "            {\"type\": \"main_ideas\", \"prompt\": main_ideas_prompt},\n",
    "            {\"type\": \"quotes\", \"prompt\": quotes_prompt},\n",
    "            {\"type\": \"passages\", \"prompt\": passages_prompt}\n",
    "        ]\n",
    "\n",
    "        system_message = \"You are a helpful assistant that is very good at problem solving who thinks step by step. You always cite direct quotes and paraphrases with the appropriate in-text citation.\"\n",
    "\n",
    "        responses = []\n",
    "        for idx, chunk in enumerate(self.chunks):\n",
    "            print(f\"Chunk {idx+1} of {len(self.chunks)}\")\n",
    "            page_responses = []\n",
    "            for prompt in prompts:\n",
    "                print(prompt[\"type\"])\n",
    "                chat = Chat(temperature=0.9, system_message=system_message)\n",
    "                response = chat(f\"CONTEXT:{chunk}\\n\\nQUERY:{prompt['prompt']}\")\n",
    "                page_responses.append(\n",
    "                    {\"Chunk\": idx+1, \"prompt_type\": prompt[\"type\"], \"response\": response})\n",
    "            responses.append(page_responses)\n",
    "        self.link_1_responses = responses\n",
    "\n",
    "        print(\"Link 1 Complete\")\n",
    "        return self\n",
    "    \n",
    "    def print_link_1(self):\n",
    "        for page in self.link_1_responses:\n",
    "            for response in page:\n",
    "                print(f'Chunk: {response[\"Chunk\"]}\\nType: {response[\"prompt_type\"]}\\n\\nResponse:\\n{response[\"response\"]}')\n",
    "    \n",
    "    def link_2(self):\n",
    "        print(\"Starting Link 2:\")\n",
    "        llm = Completion(temperature=0.9, max_tokens=1000)\n",
    "        summary_responses = []\n",
    "        for page in self.link_1_responses:\n",
    "            combine_prompt = f\"Combine the following Main Ideas:\\n{page[0]['response']}\\n\\nQuotes:\\n{page[1]['response']}\\n\\nPassages:\\n{page[2]['response']}\\n\\ninto a coherent writing. Retain any in-text citations, don't add any new citations except for {self.citation}\\n\\nSUMMARY:\"\n",
    "            summary_response = llm(combine_prompt)\n",
    "            summary_responses.append(summary_response)\n",
    "        self.link_2_responses = summary_responses\n",
    "        print(\"Link 2 Complete\")\n",
    "        return self\n",
    "    \n",
    "    def print_link_2(self):\n",
    "        for response in self.link_2_responses:\n",
    "            print(response)\n",
    "\n",
    "    def link_3(self):\n",
    "        print(\"Starting Link 3:\")\n",
    "        llm = Completion(temperature=0.9, max_tokens=1000)\n",
    "        prompt = f\"combine the following passages:{' '.join([summary for summary in self.link_2_responses])} into an essay. Retain your in-text citations and make sure to include a reference list at the end of your essay using this citation: {self.citation}. Make sure you dont repeat anything.\"\n",
    "        response = llm(prompt)\n",
    "        self.link_3_response = response\n",
    "        print(\"Link 3 Complete\")\n",
    "        return self\n",
    "    \n",
    "    def print_link_3(self):\n",
    "        print(self.link_3_response)\n",
    "\n",
    "    def link_4(self):\n",
    "        with open(\"../data/apa_guidelines.txt\", \"r\") as f:\n",
    "            guidelines = f.read()\n",
    "\n",
    "        print(\"Starting Link 4:\")\n",
    "        llm = Chat(temperature=0.9)\n",
    "        guidelines = \"https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/general_format.html\"\n",
    "        prompt = f\"Essay:{self.link_3_response}\\n\\nFinalize thee essay based on the following APA guidelines: {guidelines}\"\n",
    "        response = llm(prompt)\n",
    "        self.link_4_response = response\n",
    "        print(\"Link 4 Complete\")\n",
    "        return self\n",
    "    \n",
    "    def print_link_4(self):\n",
    "        print(self.link_4_response)\n",
    "\n",
    "    def chain(self):\n",
    "        start = time.perf_counter()\n",
    "        print(\"Starting Chain\")\n",
    "        self.link_1().link_2().link_3().link_4()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        print(f\"Chain Complete in {elapsed:0.2f} seconds.\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"../data/powers2017.pdf\"\n",
    "fpath = \"../data/moore-et-al-2022.pdf\"\n",
    "doc = Mongodoc(fpath).process_doc().get_chunks(2800).get_citation(\"APA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Link 1:\n",
      "Chunk 1 of 5\n",
      "main_ideas\n",
      "quotes\n",
      "passages\n",
      "Chunk 2 of 5\n",
      "main_ideas\n",
      "quotes\n",
      "passages\n",
      "Chunk 3 of 5\n",
      "main_ideas\n",
      "quotes\n",
      "passages\n",
      "Chunk 4 of 5\n",
      "main_ideas\n",
      "quotes\n",
      "passages\n",
      "Chunk 5 of 5\n",
      "main_ideas\n",
      "quotes\n",
      "passages\n",
      "Link 1 Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Chain at 0x7fc34af954f0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = Chain(doc)\n",
    "chain.link_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Link 2:\n",
      "Link 2 Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Chain at 0x7fc34af954f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.link_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Link 3:\n",
      "Link 3 Complete\n",
      "Starting Link 4:\n",
      "Link 4 Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Chain at 0x7fc34af954f0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.link_3()\n",
    "chain.link_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ketamine and esketamine are non-competitive non-selective antagonists of the N-methyl-D-aspartate (NMDA) receptor for glutamate, which have been repurposed as off-label treatments for depression and other psychiatric disorders. These drugs block the NMDA receptor, leading to varied changes in perception, mood, behavior, and sedation. Moore et al. (2022) provide an extensive review of the efficacy and safety of NMDA receptor antagonists for depression, with a focus on ketamine and esketamine. The authors assess the clinical trials establishing the effectiveness of ketamine for depression, treatment-resistant depression, and bipolar depression. While ketamine and esketamine have been shown to be effective in reducing depression scores, there are still substantial questions about their safety and effectiveness for psychiatric disorders.\n",
      "\n",
      "Toxicology studies of esketamine were conducted to support its FDA approval in 2019, but these studies were limited to a single dose in adult rats and did not evaluate long-term effects and risks of repeated dosing of NMDA antagonists. Furthermore, due to the altered mental state that can result from the use of these drugs, and the potential for apoptotic cell death with extended use, there is an associated risk of tolerance, overdose, addiction, and adverse effects on cognition with NMDA receptor antagonists. Research has suggested that chronic exposure to ketamine can lead to permanent impairment of brain functions in adolescent cynomolgus monkeys, as well as cortical thickness changes in chronic ketamine users. There are also potential side effects associated with ketamine use in depression, including difficulty with attention, concentration, and memory (Short et al., 2018; Wajs et al., 2020).\n",
      "\n",
      "It is clear that more research is needed to understand the safety and efficacy of ketamine and esketamine for psychiatric disorders, and to ensure appropriate clinical use with proper safety precautions on a public health and regulatory level. The limitations of previous clinical trials and the lack of evidence for sustained benefit beyond 28 days highlight the need for further investigation. It is also essential that the risks associated with these drugs are thoroughly evaluated, and proper safety measures are in place to minimize potential harm to patients.\n",
      "\n",
      "In conclusion, while ketamine and esketamine have been shown to be effective in reducing depressive symptoms, there are still significant questions regarding their safety and efficacy for treating psychiatric disorders. More research is needed to understand the long-term effects and risks of NMDA receptor antagonists, their potential for addiction and overdose, as well as the appropriateness of their clinical use. It is critical that both clinicians and regulatory agencies remain vigilant in evaluating and monitoring the use of these drugs to ensure patient safety.\n",
      "\n",
      "References:\n",
      "\n",
      "Moore, T. J., Furberg, C. D., & Mattison, D. R. (2022). Safety and effectiveness of NMDA receptor antagonists for depression: A multidisciplinary review [PDF file]. Pharmacotherapy, 42, 567-579.\n",
      "\n",
      "Short, R., Lumsden, J., & Kirkham, K. (2018). Is ketamine a safe and effective treatment for depression? A systematic review. Journal of Psychiatric Research, 100, 1-10.\n",
      "\n",
      "Wajs, C., Charbonneau, S., Suckow, S., & Belzile, E. (2020). Efficacy and safety of esketamine in depressive disorders [PDF file]. Canadian Family Physician, 66(5), e264-e272.\n"
     ]
    }
   ],
   "source": [
    "chain.print_link_4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d30dbc225a77f624d100d49fcf625dcab10209cefe14c68d9c5591ea408b94d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
